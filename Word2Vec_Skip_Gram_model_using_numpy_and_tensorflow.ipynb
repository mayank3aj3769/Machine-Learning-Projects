{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d7050b-a87a-4954-adf9-afb9cf6d058a",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: Mayank Raj\n",
    "#### Stevens ID: 20010945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7ac70-1da0-4432-839d-6d65ce4edf35",
   "metadata": {},
   "source": [
    "## Part B: Word2Vec (50 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the data loading, preprocessing, tokenization, and word pair extraction.\n",
    "3. Implement skip-gram and evaluation metrics with MLP.\n",
    "5. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95a75c8-30c8-4ca6-ac9d-42c90d32b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterlab in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from -r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from -r requirements.txt (line 3)) (3.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from -r requirements.txt (line 4)) (1.23.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from -r requirements.txt (line 5)) (1.1.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 6)) (2.10.0)\n",
      "Requirement already satisfied: tensorflow_addons in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from -r requirements.txt (line 7)) (0.19.0)\n",
      "Requirement already satisfied: nbclassic in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (4.11.1)\n",
      "Requirement already satisfied: tomli in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: ipython in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (8.4.0)\n",
      "Requirement already satisfied: jupyterlab-server~=2.10 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.16.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.2)\n",
      "Requirement already satisfied: jupyter-server<3,>=1.16.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: jinja2>=2.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: notebook<7 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.4.12)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (4.37.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (9.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (20210226132247)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (2.10.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (1.42.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (63.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (3.17.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow->-r requirements.txt (line 6)) (14.0.6)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorflow_addons->-r requirements.txt (line 7)) (2.13.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 6)) (0.35.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (21.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.13.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.14.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (6.4.4)\n",
      "Requirement already satisfied: nbformat>=5.2.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.5.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (7.3.5)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.58.0)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 1)) (302)\n",
      "Requirement already satisfied: json5 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (0.9.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: babel in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (4.11.3)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (6.9.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (1.5.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from click->nltk->-r requirements.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbclassic->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from importlib-metadata>=4.8.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (3.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jedi>=0.16->ipython->jupyterlab->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from jupyter-client>=6.1.12->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (4.11.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: testpath in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.5.13)\n",
      "Requirement already satisfied: bleach in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (4.1.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from nbformat>=5.2.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from babel->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.5)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from ipykernel->notebook<7->jupyterlab->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: executing in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 1)) (2.0.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->-r requirements.txt (line 6)) (3.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\.conda\\envs\\gputf\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.21)\n"
     ]
    }
   ],
   "source": [
    "# you may not run this cell after the first installation\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d76782-187b-4da7-baf2-d92655c79d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# If you are going to use GPU, make sure the GPU in in the output\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63231e67-b83e-4628-9c49-31b6d096b4f7",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98cc310-65bb-49dc-bd54-476f4e941f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    print('\\r' + str_, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653997a8-637a-4725-8ee5-f240913e0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb08e4-91fa-4d1e-bc92-0445397567ca",
   "metadata": {},
   "source": [
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc26198-cfbf-4c2f-89e6-460f5277c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watching time chasers it obvious that it was made by a bunch of friends', 'maybe they were sitting around one day in film school and said hey let s pool our money together and make a really bad movie or something like that', 'what ever they said they still ended up making a really bad movie dull story bad script lame acting poor cinematography bottom of the barrel stock music etc', 'all corners were cut except the one that would have prevented this film s release', 'life s like that', 'i saw this film about years ago and remember it as being particularly nasty', 'i believe it is based on a true incident a young man breaks into a nurses home and rapes tortures and kills various women it is in black and white but saves the colour for one shocking shot at the end the film seems to be trying to make some political statement but it just comes across as confused and obscene avoid', 'minor spoilersin new york joan barnard elvire audrey is informed that her husband the archeologist arthur barnard john saxon was mysteriously murdered in italy while searching an etruscan tomb', 'joan decides to travel to italy in the company of her colleague who offers his support', 'once in italy she starts having visions relative to an ancient people and maggots many maggots']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# In this imdb review, each line is a sentence seperated by a space.\n",
    "sentences = pickle.load(open(os.path.join('a2-data', 'imdb_review.pickle'), 'rb'))\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e173a8d-8f57-40e6-b278-052290ab9ffe",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization (5 Points)\n",
    "\n",
    "In this section, you will implement a Tokenizer than can record all tokens in a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b620689c-b274-483d-8246-afc3ce5364f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, sos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<unk>', mask_token='<mask>'):\n",
    "        # Special tokens. Not used in Word2Vec\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.mask_token = mask_token\n",
    "        \n",
    "        self.vocab = { sos_token: 0, eos_token: 1, pad_token: 2, unk_token: 3, mask_token: 4 }  # token -> id\n",
    "        self.inverse_vocab = { 0: sos_token, 1: eos_token, 2: pad_token, 3: unk_token, 4: mask_token }  # id -> token\n",
    "        self.token_occurrence = { sos_token: 0, eos_token: 0, pad_token: 0, unk_token: 0, mask_token: 0 }  # token -> occurrence\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" A magic method that enable program to know the number of tokens by calling:\n",
    "            ```python\n",
    "            tokenizer = Tokenizer()\n",
    "            num_tokens = len(tokenizer)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "        \n",
    "    def fit(self, sentences: List[str]):\n",
    "        \"\"\" Fit the tokenizer using all sentences.\n",
    "        1. Tokenize the sentence by splitting with spaces.\n",
    "        2. Record the occurrence of all tokens\n",
    "        3. Construct the token to index (self.vocab) map and the inversed map (self.inverse_vocab) based on the occurrence. The token with a higher occurrence has the smaller index\n",
    "        \n",
    "        Args:\n",
    "            sentences: All sentences in the dataset.\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Fitting Tokenizer:', (i + 1), '/', n)\n",
    "            # Start your code here (Step 1 and 2)\n",
    "                self.num_doc = len(sentences)        \n",
    "            tokens = sentence.lower().split()\n",
    "            for token in tokens:\n",
    "                if token in self.token_occurrence:\n",
    "                    self.token_occurrence[token] += 1\n",
    "                else:\n",
    "                    self.token_occurrence[token] = 1\n",
    "\n",
    "            # End\n",
    "        print_line('\\n #####  ')\n",
    "        # Start your code here (Step 3, update self.vocab and self.inverse_vocab)    \n",
    "        for i, token in enumerate(self.token_occurrence.keys()):\n",
    "            self.vocab[token] = i+5\n",
    "            self.inverse_vocab[i+5] = token\n",
    "        \n",
    "        print(' end of fit')\n",
    "        # End\n",
    "        \n",
    "        print('The number of distinct tokens:', len(self.vocab))\n",
    "        \n",
    "    def encode(self, sentences: List[str]) -> List[List[int]]:\n",
    "        \"\"\" Encode the sentences into token ids\n",
    "            Note: 1. if a token in a sentence does not exist in the fit encoder, we will use the <unk> token to replace it.\n",
    "                  2. If the number of tokens in a sentence is less than two, we ignore this sentence.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            sent_token_ids: A list of id list\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        sent_token_ids = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with Tokenizer:', (i + 1), '/', n)\n",
    "            token_ids = []\n",
    "            # Start your code (encode)\n",
    "            token_ids = sentence.split()\n",
    "            vocab_ids=[]\n",
    "            for k in token_ids:\n",
    "                vocab_ids.append(self.vocab[k])\n",
    "            for tk in token_ids:\n",
    "                if self.vocab.get(tk,-10)==-10:\n",
    "                    tk='<unk>'\n",
    "            if len(token_ids)>2:\n",
    "                sent_token_ids.append(vocab_ids)\n",
    "            # End\n",
    "        return sent_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61755eeb-5d1c-4923-b58a-0e9409f4b4cd",
   "metadata": {},
   "source": [
    "#### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7fec131-d49d-4045-9c69-2df685ef433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Fitting Tokenizer: 1 / 2\r",
      "Fitting Tokenizer: 2 / 2\r\n",
      " #####   end of fit\n",
      "The number of distinct tokens: 44\n",
      "\n",
      "it : 2\n",
      "that : 2\n",
      "a : 2\n",
      "and : 2\n",
      "watching : 1\n",
      "time : 1\n",
      "chasers : 1\n",
      "obvious : 1\n",
      "was : 1\n",
      "made : 1\n",
      "\n",
      "\r",
      "Encoding with Tokenizer: 1 / 2\r",
      "Encoding with Tokenizer: 2 / 2watching time chasers it obvious that it was made by a bunch of friends ['watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends'] \n",
      "\n",
      "maybe they were sitting around one day in film school and said hey let s pool our money together and make a really bad movie or something like that ['maybe', 'they', 'were', 'sitting', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'said', 'hey', 'let', 's', 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', 'or', 'something', 'like', 'that'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = sentences[:2]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(test_sentences)\n",
    "print()\n",
    "\n",
    "token_occurrence = sorted(tokenizer.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
    "for token, occurrence in token_occurrence[:10]:\n",
    "    print(token, ':', occurrence)\n",
    "print()\n",
    "sent_token_ids = tokenizer.encode(test_sentences)\n",
    "\n",
    "for test_sentence, token_ids in zip(test_sentences, sent_token_ids):\n",
    "    sentence = [tokenizer.inverse_vocab[token] for token in token_ids]\n",
    "    print(test_sentence, sentence, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24428c82-0496-4b7c-a8aa-d8ce7b4ff03f",
   "metadata": {},
   "source": [
    "Encode all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c68fc5a9-452b-44ad-8df1-b6b96c823282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tokenizer: 528199 / 528199\n",
      " #####   end of fit\n",
      "The number of distinct tokens: 102299\n",
      "Encoding with Tokenizer: 528199 / 528199"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(sentences)\n",
    "sent_token_ids = tokenizer.encode(sentences)\n",
    "\n",
    "# Note: if you implement correctly, there will be about 100k tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dea9e0d-63e2-48be-8f36-f659f262b6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token number in a sentence: 767\n",
      "min token number in a sentence: 3\n",
      "avg token number in a sentence: 23.07580687218086\n"
     ]
    }
   ],
   "source": [
    "print('max token number in a sentence:', max(map(lambda e: len(e), sent_token_ids)))\n",
    "print('min token number in a sentence:', min(map(lambda e: len(e), sent_token_ids)))\n",
    "print('avg token number in a sentence:', sum(map(lambda e: len(e), sent_token_ids)) / len(sent_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5bf4377-4c9f-4d2d-a856-3d193e01f5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 678333\n",
      "and : 329979\n",
      "a : 328341\n",
      "of : 293903\n",
      "to : 271514\n",
      "is : 214380\n",
      "it : 191411\n",
      "in : 188920\n",
      "i : 170761\n",
      "this : 150693\n"
     ]
    }
   ],
   "source": [
    "token_occurrence = sorted(tokenizer.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
    "for token, occurrence in token_occurrence[:10]:\n",
    "    print(token, ':', occurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e71bd1-478c-45e6-b679-6def46ff4829",
   "metadata": {},
   "source": [
    "## 2. Training Data Generation (10 Points)\n",
    "\n",
    "### Positive Samples (5 Points)\n",
    "In this section, you are required to generate the positive training data for skip-gram.\n",
    "\n",
    "- Given a token sequence, for every center token, you need to use a window whose size is **2** to retrieve context words as positive pairs.\n",
    "    \n",
    "For example, given the token sequence \"The quick brown fox jumps over the lazy dog .\", when the center word is \"fox\", the context words are:\n",
    "$$\\text{The}~\\underbrace{\\text{quick brown}}_\\text{context words}~\\underbrace{\\text{fox}}_{\\text{center word}}~\\underbrace{\\text{jumps over}}_\\text{context words}~\\text{the lazy dog .}$$\n",
    "We can generate four positive pairs: (fox, quick), (fox, brown), (fox, jumps), (fox, over)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44537480-4186-488f-b4e7-89d211054717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable\n",
    "\n",
    "def positive_pairs_generator(sent_token_ids: List[List[int]], window_size: int = 2) -> np.ndarray:\n",
    "    \"\"\" Generate positive pairs: (center word, context word)\n",
    "    \n",
    "    Args:\n",
    "        sent_token_ids: List of token list. Each element is a token list (sentence)\n",
    "        window_size: the context window size. You should retrieve window size context words in the left and window size context words in the right.\n",
    "        \n",
    "    Yield:\n",
    "        center_word, context_word: a positive pair\n",
    "        \n",
    "    Note: this is a generator function which yields pairs. Do not return anything.\n",
    "    \"\"\"\n",
    "    n = len(sent_token_ids)\n",
    "    for t, token_ids in enumerate(sent_token_ids):\n",
    "        if t % 100 == 0 or t == n - 1:\n",
    "            print_line('Positive Pair Generation:', (t + 1), '/', n)\n",
    "    \n",
    "        for i, center_word in enumerate(token_ids):\n",
    "            context_words = []\n",
    "            # Start your code here\n",
    "            # Append context words of center_word to context_words\n",
    "            l=len(token_ids)\n",
    "            for j in range(l):\n",
    "                if token_ids[j]!=center_word:\n",
    "                    context_words.append(token_ids[j])                \n",
    "            # End\n",
    "            for context_word in context_words:\n",
    "                yield center_word, context_word\n",
    "\n",
    "\n",
    "def save_train_file(sent_token_ids: List[List[int]], window_size: int = 2):\n",
    "    generator = positive_pairs_generator(sent_token_ids, window_size)\n",
    "    buffer_size = 100000\n",
    "    buffer = []\n",
    "    count = 0\n",
    "    with open(os.path.join('a2-data', 'word2vec_train_file.txt'), 'w') as train_file:\n",
    "        for center_word, context_words in generator:\n",
    "            count += 1\n",
    "            line = f'{center_word} {context_words}\\n'\n",
    "            buffer.append(line)\n",
    "            if len(buffer) == buffer_size:\n",
    "                random.shuffle(buffer)\n",
    "                train_file.writelines(buffer)\n",
    "                buffer = []\n",
    "        if len(buffer) > 0:\n",
    "            train_file.writelines(buffer)\n",
    "    print('\\n')\n",
    "    print(f'There are {count} pairs')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "157b7bbf-ffce-4955-903f-a23dabeff734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Pair Generation: 513898 / 513898\n",
      "\n",
      "There are 438634942 pairs\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "num_samples = save_train_file(sent_token_ids, window_size)\n",
    "\n",
    "# Note: if you implement correctly, there will be about 44M pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab2a3e-fd01-4c01-a0dc-cf795a8c887a",
   "metadata": {},
   "source": [
    "### Negative Samples (5 Points)\n",
    "\n",
    "We randomly select $K$ words from the vocabulary as negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe09b55e-52d8-47b4-a7b3-6163a1c82ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(vocab_size: int, batch_size: int, negative_sample_num: int) -> np.ndarray:\n",
    "    \"\"\" Generate negative words\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: number of tokens in the vocabulary\n",
    "        batch_size: number of samples (center word) in a batch\n",
    "        negative_sample_num: number of negative words sampled for a center word\n",
    "        \n",
    "    Return:\n",
    "        negative_words: Shape of (batch_size x negative_sample_num)\n",
    "        \n",
    "    Note: 1. You should NOT sample special token in the vocabulary, i.e., the token ids range should be [5, vocab_size)\n",
    "          2. Hint: See numpy.random.choice. Read carefully for each parameter of this function\n",
    "    \"\"\"\n",
    "    \n",
    "    negative_words = None\n",
    "    # Start your code here\n",
    "    negative_words = np.random.choice(np.arange(5,vocab_size), (batch_size,negative_sample_num))\n",
    "    \n",
    "    # End\n",
    "    return negative_words\n",
    "\n",
    "\n",
    "def train_data_generator(batch_size, vocab_size, negative_sample_num):\n",
    "    with open(os.path.join('a2-data', 'word2vec_train_file.txt')) as train_file:\n",
    "        while True:\n",
    "            batch_center_words, batch_context_words = [], []\n",
    "            for line in train_file:\n",
    "                center_word, context_word = line.strip().split()\n",
    "                batch_center_words.append(int(center_word))\n",
    "                batch_context_words.append(int(context_word))\n",
    "                if len(batch_center_words) == batch_size:\n",
    "                    negative_words = get_negative_samples(vocab_size, batch_size, negative_sample_num)\n",
    "                    yield batch_center_words, batch_context_words, negative_words\n",
    "                    batch_center_words, batch_context_words = [], []\n",
    "            train_file.seek(0)\n",
    "            if len(batch_center_words) > 0:\n",
    "                real_batch_size = len(batch_center_words)\n",
    "                negative_words = get_negative_samples(vocab_size, real_batch_size, negative_sample_num)\n",
    "                yield batch_center_words, batch_context_words, negative_words\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988ca2f-ce4a-40eb-9fef-4d93f32575da",
   "metadata": {},
   "source": [
    "## 3. Skip-gram Model (20 Points)\n",
    "\n",
    "- Loss: 10 Points\n",
    "- Model: 10 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8fefa08-6231-4fbf-869a-d07ebce350f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "def negative_sampling_loss(center_embeddings, context_embeddings, negative_embeddings):\n",
    "    \"\"\" Calculate the negative sampling loss\n",
    "    \n",
    "    Args:\n",
    "        center_embeddings: v_c, (batch_size x embedding_dim)\n",
    "        context_embeddings: u_o, (batch_size x embedding_dim)\n",
    "        negative_embeddings: u_k, (batch_size x negative_sample_num x embedding_dim)\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    # Start your code here\n",
    "    # 1. Calculate positive dot product\n",
    "    # 2. loss for the positive pairs\n",
    "    # 3. Calculate negative dot product\n",
    "    # 4. loss for the negative words\n",
    "    # Hint: See tf.reduce_sum, tf.expand_dims, tf.reduce_mean for help\n",
    "    \n",
    "    dot_product = tf.reduce_sum(tf.multiply(context_embeddings, center_embeddings), axis=1)\n",
    "    positive = tf.nn.sigmoid(dot_product) #positive dot product\n",
    "    loss_positive = tf.math.log(positive) #loss for positive pair\n",
    "    \n",
    "    center_embeddings = tf.expand_dims(center_embeddings, axis=1)\n",
    "    dot_product = tf.reduce_sum(tf.multiply(negative_embeddings, center_embeddings), axis=2)\n",
    "    negative = tf.nn.sigmoid(-dot_product) \n",
    "    loss_negative = tf.reduce_sum(tf.math.log(negative), axis=1) #loss for negative pair\n",
    "\n",
    "    loss = -(tf.reduce_mean(loss_positive + loss_negative)) #net loss\n",
    "    # End\n",
    "    return loss\n",
    "\n",
    "class SkipGram(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\" Skip-gram model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Start your code here\n",
    "        # Initialize embedding layers\n",
    "        \n",
    "        self.center_embeddings = tf.keras.layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "        self.context_embeddings =tf.keras.layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=negative_sample_num+1)\n",
    "        self.negative_embeddings=tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
    "                                       input_length=negative_sample_num+1)\n",
    "        # Hint: See tf.keras.layers.Embedding\n",
    "\n",
    "        # End\n",
    "        \n",
    "    def call(self, center_words, context_words, negative_words):\n",
    "        \"\"\" Forward of the skip-gram model\n",
    "        \n",
    "        Args:\n",
    "            center_words: tensor (batch_size, )\n",
    "            context_words: tensor (batch_size, )\n",
    "            negative_words: tensor (batch_size, negative_embeddings)\n",
    "            \n",
    "        Return:\n",
    "            center_embeddings, context_embeddings, negative_embeddings: The input for the negative_sampling_loss.\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        center_embeddings=self.center_embeddings(center_words)\n",
    "        context_embeddings=self.context_embeddings(context_words)\n",
    "        negative_embeddings=self.negative_embeddings(negative_words)\n",
    "\n",
    "        # End\n",
    "        \n",
    "        return center_embeddings, context_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cec8205b-1449-44dc-81ac-0dee1f155676",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 64\n",
    "num_epoch = 2\n",
    "batch_size = 1024\n",
    "negative_sample_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f0e3ff1-1f93-46bd-b849-42a0ead9ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27449114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ee3aa8e-eed3-49fd-85cc-f6afb06d9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2 - Step 428355 / 428355 - loss: 0.01050\n",
      "Epoch 2 / 2 - Step 428355 / 428355 - loss: 0.00010\n"
     ]
    }
   ],
   "source": [
    "n_batch = int(np.ceil(num_samples / batch_size))\n",
    "train_gen = train_data_generator(batch_size, vocab_size, negative_sample_num)\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx in range(n_batch):\n",
    "        batch = next(train_gen)\n",
    "        real_batch_size = len(batch[0])\n",
    "        batch = [tf.convert_to_tensor(d, tf.int64) for d in batch]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(*batch)\n",
    "            loss = negative_sampling_loss(*output)\n",
    "\n",
    "        if batch_idx % 10 == 0 or batch_idx == num_samples - 1:\n",
    "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_batch} - loss: {loss:.4f}')\n",
    "            \n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        epoch_loss += loss * real_batch_size\n",
    "    print(f'\\rEpoch {epoch + 1} / {num_epoch} - Step {n_batch} / {n_batch} - loss: {epoch_loss / num_samples:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70839bdf-2b75-4219-9543-4e2ccdd0ee9e",
   "metadata": {},
   "source": [
    "## 4. Visualization (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "01170df8-363e-4250-9c28-d2095af76363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102299, 64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_vecs = tf.concat([model.center_embeddings.weights[0], model.context_embeddings.weights[0]], axis=-1).numpy()\n",
    "center_vecs = model.center_embeddings.weights[0].numpy()\n",
    "center_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3f58dc6-ce6c-4b7a-bc41-cad74addd619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "visualize_words = [\n",
    "    \"movie\", \"film\", \"story\",\n",
    "    \"good\", \"enjoyable\", \"great\", \"bad\",\n",
    "    \"coffee\", \"tea\", \"milk\"\n",
    "]\n",
    "visualize_idx = [tokenizer.vocab[word] for word in visualize_words]\n",
    "visualize_vecs = center_vecs[visualize_idx, :]\n",
    "\n",
    "temp = (visualize_vecs - np.mean(visualize_vecs, axis=0))\n",
    "covariance = 1.0 / len(visualize_idx) * temp.T.dot(temp)\n",
    "U, S, V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:, 0:2])\n",
    "\n",
    "for i in range(len(visualize_words)):\n",
    "    plt.text(coord[i, 0], coord[i, 1], visualize_words[i],\n",
    "             bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:, 0]), np.max(coord[:, 0])))\n",
    "plt.ylim((np.min(coord[:, 1]), np.max(coord[:, 1])))\n",
    "\n",
    "plt.savefig('word_vectors.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21566d6c-998e-4b86-b765-9b7704e94737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(vec, mat, k):\n",
    "    \"\"\" Implement the KNN algorithm based on cosine similarity, which will be used for analysis.\n",
    "\n",
    "        Args:\n",
    "            vec: numpy ndarray, the target vector\n",
    "            mat: numpy ndarray, a matrix contains all the vectors (each row is a vector)\n",
    "            k: the number of the nearest neighbors you want to find.\n",
    "            \n",
    "        Return:\n",
    "            indices: the k indices of the matrix's rows that are closest to the vec\n",
    "    \"\"\"\n",
    "    indicies = []\n",
    "    def cosine_similarity(vector_1: np.ndarray, vector_2: np.ndarray) -> float:\n",
    "    \n",
    "        return np.dot(vector_1, vector_2)/(np.linalg.norm(vector_1) *\n",
    "                                          np.linalg.norm(vector_2))\n",
    "    # Start your code here\n",
    "    # Note: DO NOT use for loop to calculate the similarity between two vectors. You are required to vectorize the calculation.\n",
    "    # Hint: See np.argsort\n",
    "    indices=np.argsort([cosine_similarity(vec, row)\n",
    "                          for row in mat])[-k:]\n",
    "\n",
    "    # End\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1433fc0-5935-49d1-9cc8-c232ae7baff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: \"movie\" is close to ['rothrock', 'staggers', 'crown', 'quarrel', 'assumption', 'tributes', 'harlot', 'ingenue', 'sculpted', 'movie']\n",
      "Word: \"film\" is close to ['performing', 'topaz', 'damascus', 'achieve', 'jacob', 'psychopaths', 'planks', 'shrug', 'softens', 'film']\n",
      "Word: \"story\" is close to ['matheson', 'rounds', 'depending', 'manual', 'miscommunication', 'succumbed', 'weasel', 'missing', 'loosing', 'story']\n",
      "Word: \"good\" is close to ['killings', 'effective', 'garr', 'inherits', 'duffy', 'spike', 'mates', 'pictures', 'concert', 'good']\n",
      "Word: \"enjoyable\" is close to ['schwarzenegger', 'awry', 'transcends', 'akins', 'pleasures', 'argue', 'unwanted', 'firing', 'highlander', 'enjoyable']\n",
      "Word: \"great\" is close to ['unrealistically', 'insulted', 'mere', 'hijack', 'croatia', 'alcatraz', 'milius', 'carroll', 'posh', 'great']\n",
      "Word: \"bad\" is close to ['rojo', 'liabilities', 'absurd', 'disguise', 'flows', 'tug', 'odious', 'dosed', 'dictated', 'bad']\n",
      "Word: \"coffee\" is close to ['occupies', 'pistol', 'housewife', 'engage', 'uncertain', 'pushed', 'viggo', 'swank', 'takashi', 'coffee']\n",
      "Word: \"tea\" is close to ['crass', 'gowns', 'odyssey', 'simplicity', 'bounty', 'industrialist', 'jacket', 'nichols', 'mortensen', 'tea']\n",
      "Word: \"milk\" is close to ['racket', 'loads', 'solved', 'steps', 'takashi', 'grissom', 'hampered', 'nino', 'ticket', 'milk']\n"
     ]
    }
   ],
   "source": [
    "for word in visualize_words:\n",
    "    idx = tokenizer.vocab[word]\n",
    "    vec = center_vecs[idx]\n",
    "    indices = knn(vec, center_vecs, 10)\n",
    "    closed_words = [tokenizer.inverse_vocab[i] for i in indices]\n",
    "    print('Word: \"{}\" is close to {}'.format(word, closed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea05615-3aa9-4b17-89ee-40ef2eff0bcf",
   "metadata": {},
   "source": [
    "## 5. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa38684-e334-4958-8fe9-ac98cfbc7133",
   "metadata": {},
   "source": [
    "1)- Negative sampling loss significantly brings down the computation cost and time.\n",
    "\n",
    "2)- Use of iterators is more memory efficient and helps with large datasets. \n",
    "\n",
    "2)-Similarity between terms in knn based algorithm is calculated using cosine function which is more accurate than euclidean distance for text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73934c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
